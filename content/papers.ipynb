{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a16f4c",
   "metadata": {},
   "source": [
    "# Text mining\n",
    "\n",
    "## List of papers\n",
    "\n",
    "We first assembled the title, the name of the corresponding author, and the abstract for all the articles into a tabular-separated values (tsv) file, which we publicly archived on [Figshare](https://doi.org/10.6084/m9.figshare.5497468.v2). We use the [Repo2Data](https://github.com/SIMEXP/Repo2Data) tool developed by the NeuroLibre team to collect these data and include them in our reproducible computational environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e37ef3",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Check where the notebooks are located in the current path\n",
    "# This is necessary because thebe does not behave like jupyter hub\n",
    "import os\n",
    "if os.path.exists('content'):\n",
    "  os.chdir('content')\n",
    "\n",
    "# Import data from figshare, using repo2data https://github.com/SIMEXP/Repo2Data\n",
    "from repo2data import repo2data\n",
    "dl = repo2data.Repo2Data(os.path.join('..', 'binder', 'data_requirement.json'))\n",
    "path_data = dl.install()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dd99c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "data = pd.read_csv(os.path.join(path_data, '9843721'), sep='\\t',header=0)\n",
    "# Show us the data!\n",
    "pd.options.display.max_rows = 5\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a96fb7",
   "metadata": {},
   "source": [
    "## Word features\n",
    "For each paper, we used [scikit-learn](http://scikit-learn.org) {cite:p}`Kramer2016` to extract a bag of words representation for each abstract, picking on the 300 most important terms seen across all articles based on a term frequency-inverse document frequency [(tf-idf) index](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Following that, a special value decomposition was used to further reduce the dimensionality of the abstracts to 10 components. We ended up with a component matrix of dimension 38 (articles) times 10 (abstract text components). The distribution of each of the 38 articles across the 10 components is represented below. Note how some articles have particular high loadings on specific components, suggesting these may capture particular topics. Rather than visually inspect the component loadings to group paper ourselves, we are going to resort to an automated parcellation (clustering) technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5f492",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Prepare a model to extract a bag of words representation\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=300,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(data.Abstract) # apply the TFIDF model to the abstracts of the papers\n",
    "fts = vectorizer.get_feature_names_out() # Get the most relevant terms selected by the procedure\n",
    "svd = TruncatedSVD(10, algorithm=\"arpack\") # Prepare the SVD model\n",
    "normalizer = Normalizer(copy=False) # Normalize the outputs of the svd\n",
    "lsa = make_pipeline(svd, normalizer) # Put the SVD and normalization in a pipeline\n",
    "X = lsa.fit_transform(X) # Apply the SVD to the TFIDF features\n",
    "\n",
    "# Now make a radar plot with all the components\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "df_X = pd.DataFrame(data=np.concatenate([data['Author'].to_numpy().reshape([38, 1]), X], axis=1),\n",
    "                    columns= np.concatenate([['First author'],[f'component {d}' for d in range(10)]]))\n",
    "df_visu = pd.melt(df_X, id_vars='First author')\n",
    "fig = px.line_polar(df_visu, r='value', theta='variable', color='First author', line_close=True,\n",
    "            color_discrete_sequence=px.colors.sequential.Plasma_r)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63059c06",
   "metadata": {},
   "source": [
    "## Parcellate the papers\n",
    "\n",
    "Now that the content of each paper has been condensed into only 10 (hopefully informative) numbers, we can run these features into a trusted, classic parcellation algorithm: Ward's agglomerative hierarchical clustering, as implemented in the scipy library. We cut the hierarchy to extract 7 \"paper parcels\", and also use the hierarchy to re-order the papers, such that similar papers are close in order, as illustrated in a dendrogram representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1d7cb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree\n",
    "n_clusters = 7 # Set the number of clusters\n",
    "\n",
    "# Make figures nice and big\n",
    "fig_size = [15,12]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "Xd = pd.DataFrame(X,index=range(0, data.shape[0])) # Convert the data to a pandas frame\n",
    "hier = linkage(Xd, method='ward', metric='euclidean') # scipy's hierarchical clustering\n",
    "res = dendrogram(hier, labels=data['Author'].to_numpy(), get_leaves=True) # Generate a dendrogram from the hierarchy\n",
    "order = res.get('leaves') # Extract the order on papers from the dendrogram\n",
    "\n",
    "# Cut the hierarchy and turn the parcellation into a dataframe\n",
    "part = np.squeeze(cut_tree(hier,n_clusters=n_clusters))\n",
    "part = pd.DataFrame(data=part[order],columns=[\"Parcel\"],index=order)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5cb04",
   "metadata": {},
   "source": [
    "## Similarity matrix\n",
    "\n",
    "So, to get a better feel of the similarity between papers that was fed into the clustering procedure, we extracted the 38x38 (papers x papers) correlation matrix across features. Papers are re-ordered in the matrix according to the above hierarchy. Each \"paper parcel\" has been indicated by a white square along the diagonal, which represents the similarity measures between papers falling into the same parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c39e5d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize figure/axes\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "# Generate and reorder the correlation matrix\n",
    "R = Xd.transpose().corr()\n",
    "Rs = R.iloc[order,order]\n",
    "n = len(order)\n",
    "\n",
    "# Show the matrix with seaborn\n",
    "import seaborn as sns\n",
    "sns.heatmap(Rs, square=True )\n",
    "\n",
    "# I somehow have not been able to find a good tool to do that\n",
    "# So a bit of an ugly hack to add diagonal squares\n",
    "# that highlight each cluster\n",
    "val , ind = scipy.unique(part,return_index=True)\n",
    "ind = scipy.sort(ind)\n",
    "ind = np.append(ind,Rs.shape[0])\n",
    "ind = ind\n",
    "for ii in range(0,ind.shape[0]-1):\n",
    "    p = patches.Rectangle((ind[ii+1] , ind[ii+1]),\n",
    "                           ind[ii]-ind[ii + 1],\n",
    "                           ind[ii]-ind[ii + 1],\n",
    "                           angle=0,\n",
    "                           edgecolor='white',\n",
    "                           facecolor='none' ,\n",
    "                           linewidth=3)\n",
    "    ax.add_patch(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9d024",
   "metadata": {},
   "source": [
    "## Word cloud\n",
    "Now, each paper of the special issue has been assigned to one and only one out of 7 possible \"paper parcel\". For each paper parcel, we can evaluate which words contribute more to the dominant component associated with that parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce055d6d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "max_comp = 2 # Number of components to examine\n",
    "max_words = 10 # Number of words to include per component\n",
    "# Compute the average / std across all components\n",
    "gavg = Xd.transpose()\n",
    "gavg_mean = gavg.mean(axis=1)\n",
    "gavg_std = gavg.std(axis=1)\n",
    "\n",
    "# Initialize a frame to store the dominant words for each parcel\n",
    "all_comp = []\n",
    "words = pd.DataFrame(columns=[\"Parcel\",\"word\",\"weight\"])\n",
    "nb_words = 0\n",
    "\n",
    "for cc in range(0,n_clusters): # Loop over parcels\n",
    "    avg = Xd[part.Parcel==cc] # average components inside the parcel\n",
    "\n",
    "    # Normalize from the grand average\n",
    "    avg = avg.transpose()\n",
    "    avg = avg.mean(axis=1)\n",
    "    avg = np.divide((avg-gavg_mean),gavg_std)\n",
    "\n",
    "    # Look for the component with highest contributions\n",
    "    # There could in theory be components with negative contributions as well\n",
    "    # In practice here this never happens\n",
    "    val_avg , ind_avg = np.unique(-  avg ,return_index=True)\n",
    "    for pos in range(0,max_comp): # We will look at the top components only\n",
    "        val , ind = np.unique(- svd.components_[ind_avg[pos]],return_index=True) # Rank the components\n",
    "        ind = ind[0:max_words] # Just retain the first 6 words\n",
    "        for ww in ind: # Add the words and associated parcels / weights to the table\n",
    "            nline = pd.DataFrame([[cc,fts[ww],np.round(100*svd.components_[ind_avg[0]][ww])]],\n",
    "                            columns=[\"Parcel\",\"word\",\"weight\"],\n",
    "                            index=[nb_words])\n",
    "            words = pd.concat([words, nline])\n",
    "            nb_words = nb_words+1\n",
    "\n",
    "# It's word cloud time!\n",
    "from wordcloud import WordCloud\n",
    "import imageio\n",
    "\n",
    "n_parcels = int(words[\"Parcel\"].max())+1\n",
    "plt.clf()\n",
    "word_width = 1000\n",
    "word_height = 1000\n",
    "fig = plt.figure(figsize=(7, 7), dpi=300)\n",
    "for pp in range(0, n_parcels): # Loop over parcels\n",
    "    dd = {}\n",
    "    ind = words.loc[words[\"Parcel\"]==pp].index # Select the words in selected parcel\n",
    "\n",
    "    # Create a dictionary, where each word is associated to its weight\n",
    "    for ii in ind:\n",
    "        dd[words[\"word\"][ii]] =  (words[\"weight\"][ii] / words[\"weight\"][ind[0]])\n",
    "\n",
    "    mask = imageio.imread(os.path.join('numbers', f'{pp + 1}.png'))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        random_state=0,\n",
    "        relative_scaling=0,\n",
    "        max_font_size=400,\n",
    "        mask=mask[:, :, 3]-1,\n",
    "        contour_width=5,\n",
    "        contour_color='steelblue'\n",
    "    ).generate_from_frequencies(dd)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.subplot(3, 3, pp+1)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e2df2",
   "metadata": {},
   "source": [
    "## Categories\n",
    "Thanks to the word clouds, these simple data-driven categories turned out to be fairly easily interpretable. For example, the word cloud of the category number 4 features prominently words like \"white\", \"matter\" and \"bundles\". If we examine the exact list of papers included in this category, we see that it is composed of four papers, which all considered parcels derived from white matter bundles with diffusion imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748e2cc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ask pandas to display full titles for the papers\n",
    "pd.set_option('max_colwidth', 0)\n",
    "# zero-indexing beware! parcel number 4 is filled with 3 (!)\n",
    "category_4 = part.index[part['Parcel'] == 3]\n",
    "tmp = data.iloc[category_4]\n",
    "tmp[['Article', 'Author']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0515d1",
   "metadata": {},
   "source": [
    "We can also check the distribution of component loadings for this category alone. As expected, there is a certain similarity in the component loadings for these papers, in particular along `component 4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa76ce3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "df_visu = pd.melt(df_X.iloc[category_4], id_vars='First author')\n",
    "fig = px.line_polar(df_visu, r='value', theta='variable', color='First author', line_close=True,\n",
    "            color_discrete_sequence=px.colors.sequential.Plasma_r)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fcc8d",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
